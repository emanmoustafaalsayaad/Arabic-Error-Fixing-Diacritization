{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e363fd09",
   "metadata": {},
   "source": [
    "# Arabic GEC Pipeline (Kaggle Optimized)\n",
    "\n",
    "This notebook implements the QALB 2014  correction pipeline, adapted for Kaggle Kernels.\n",
    "\n",
    "**Steps:**\n",
    "1.  Setup & Data Download\n",
    "2.  M2 Format Parsing (Train + Dev)\n",
    "3.  Fine-tuning AraT5 on Kaggle GPU\n",
    "4.  Inference & Export\n",
    "\n",
    "**Note:** Ensure you have selected **GPU T4 x2** or **P100** from the accelerator menu in Kaggle settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5953d407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T21:55:35.591689Z",
     "iopub.status.busy": "2026-01-12T21:55:35.591512Z",
     "iopub.status.idle": "2026-01-12T21:55:35.598870Z",
     "shell.execute_reply": "2026-01-12T21:55:35.598235Z",
     "shell.execute_reply.started": "2026-01-12T21:55:35.591674Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh environment.\n"
     ]
    }
   ],
   "source": [
    "# Clean environment (optional, but good for retries)\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"/kaggle/working/qalb_dataset.zip\"):\n",
    "    print(\"Previous execution detected. You might want to skip download.\")\n",
    "else:\n",
    "    print(\"Fresh environment.\")\n",
    "\n",
    "# Make sure we are in the working directory\n",
    "os.chdir(\"/kaggle/working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f61429",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T21:55:39.414019Z",
     "iopub.status.busy": "2026-01-12T21:55:39.413746Z",
     "iopub.status.idle": "2026-01-12T21:55:50.841287Z",
     "shell.execute_reply": "2026-01-12T21:55:50.840430Z",
     "shell.execute_reply.started": "2026-01-12T21:55:39.413998Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\n",
      "Requirement already satisfied: pyarabic in /usr/local/lib/python3.11/dist-packages (0.6.15)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.6.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from pyarabic) (1.17.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.6.0-py3-none-any.whl (100 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: pyarrow, portalocker, sacrebleu, evaluate\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 19.0.1\n",
      "    Uninstalling pyarrow-19.0.1:\n",
      "      Successfully uninstalled pyarrow-19.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.6 portalocker-3.2.0 pyarrow-22.0.0 sacrebleu-2.6.0\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers datasets pyarabic gdown sentencepiece evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff4a72",
   "metadata": {},
   "source": [
    "## 1. Data Download\n",
    "Downloads the dataset directly to the ephemeral storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c9a8ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T21:55:55.432256Z",
     "iopub.status.busy": "2026-01-12T21:55:55.431966Z",
     "iopub.status.idle": "2026-01-12T21:56:00.004785Z",
     "shell.execute_reply": "2026-01-12T21:56:00.004022Z",
     "shell.execute_reply.started": "2026-01-12T21:55:55.432228Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1hvLiiMvvubyCEAZK4KIWgu7qHBNCHOp-\n",
      "From (redirected): https://drive.google.com/uc?id=1hvLiiMvvubyCEAZK4KIWgu7qHBNCHOp-&confirm=t&uuid=081305af-d859-4c85-96f4-00ffb99e53ef\n",
      "To: /kaggle/working/qalb_dataset.zip\n",
      "100%|██████████| 94.3M/94.3M [00:00<00:00, 102MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted.\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Download the file from Google Drive (Public Link)\n",
    "file_id = '1hvLiiMvvubyCEAZK4KIWgu7qHBNCHOp-'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "output_file = 'qalb_dataset.zip'\n",
    "\n",
    "# Only download if not exists\n",
    "if not os.path.exists(output_file):\n",
    "    print(\"Downloading dataset...\")\n",
    "    gdown.download(url, output_file, quiet=False)\n",
    "\n",
    "# Unzip the file\n",
    "if os.path.exists(output_file) and not os.path.exists(\"QALB-0.9.1-Dec03-2021-SharedTasks\"):\n",
    "    # Extract to current directory to find it easily\n",
    "    with zipfile.ZipFile(output_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "    print(\"Dataset extracted.\")\n",
    "else:\n",
    "    print(\"Dataset already extracted or download failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47636737",
   "metadata": {},
   "source": [
    "## 2. Step 1: The M2 Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0198595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T21:56:05.761269Z",
     "iopub.status.busy": "2026-01-12T21:56:05.760672Z",
     "iopub.status.idle": "2026-01-12T21:56:07.041250Z",
     "shell.execute_reply": "2026-01-12T21:56:07.040658Z",
     "shell.execute_reply.started": "2026-01-12T21:56:05.761246Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./QALB-0.9.1-Dec03-2021-SharedTasks/data/2014/train/QALB-2014-L1-Train.m2...\n",
      "Processing ./QALB-0.9.1-Dec03-2021-SharedTasks/data/2014/dev/QALB-2014-L1-Dev.m2...\n",
      "Saved 20428 pairs to qalb_full_gec.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def parse_m2_and_generate_csv(m2_paths, output_csv_path):\n",
    "    all_processed_data = []\n",
    "\n",
    "    for m2_path in m2_paths:\n",
    "        print(f\"Processing {m2_path}...\")\n",
    "        if not os.path.exists(m2_path):\n",
    "            print(f\"File not found: {m2_path}\")\n",
    "            continue\n",
    "\n",
    "        with open(m2_path, 'r', encoding='utf-8') as f:\n",
    "            m2_data = f.read().strip().split(\"\\n\\n\")\n",
    "\n",
    "        for entry in m2_data:\n",
    "            lines = entry.split(\"\\n\")\n",
    "            if not lines:\n",
    "                continue\n",
    "\n",
    "            # The first line starts with 'S' and contains the original sentence (tokenized)\n",
    "            source_line = lines[0]\n",
    "            if not source_line.startswith(\"S \"):\n",
    "                continue\n",
    "\n",
    "            original_tokens = source_line[2:].split()\n",
    "            edits = []\n",
    "\n",
    "            # Subsequent lines start with 'A' and contain edits\n",
    "            for line in lines[1:]:\n",
    "                if line.startswith(\"A \"):\n",
    "                    parts = line[2:].split(\"||\")\n",
    "                    # Format: A start_off end_off||type||correction||...\n",
    "                    span = parts[0].split()\n",
    "                    start_off = int(span[0])\n",
    "                    end_off = int(span[1])\n",
    "\n",
    "                    # Clean up correction: Remove remaining '|' from split and whitespace\n",
    "                    correction = parts[2].replace(\"|\", \"\").strip()\n",
    "\n",
    "                    edits.append((start_off, end_off, correction))\n",
    "\n",
    "            # Critical Reversal Logic: Sort edits by start_off in descending order\n",
    "            edits.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            corrected_tokens = list(original_tokens)\n",
    "            for start, end, subst in edits:\n",
    "                if subst == \"-NONE-\":\n",
    "                    replacement = []\n",
    "                else:\n",
    "                    replacement = subst.split()\n",
    "\n",
    "                corrected_tokens[start:end] = replacement\n",
    "\n",
    "            original_sent = \" \".join(original_tokens)\n",
    "            corrected_sent = \" \".join(corrected_tokens)\n",
    "\n",
    "            all_processed_data.append([original_sent, corrected_sent])\n",
    "\n",
    "    # Save to CSV\n",
    "    with open(output_csv_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"incorrect\", \"correct\"])\n",
    "        writer.writerows(all_processed_data)\n",
    "\n",
    "    print(f\"Saved {len(all_processed_data)} pairs to {output_csv_path}\")\n",
    "\n",
    "# Find ALL M2 files (Train + Dev)\n",
    "m2_files_found = []\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for file in files:\n",
    "        # Look for both Train and Dev files to get maximum data\n",
    "        # Filter for 2014 specific files as requested\n",
    "        if file.endswith(\".m2\") and (\"Train\" in file or \"Dev\" in file) and \"2014\" in file:\n",
    "             m2_files_found.append(os.path.join(root, file))\n",
    "\n",
    "if m2_files_found:\n",
    "    parse_m2_and_generate_csv(m2_files_found, \"qalb_full_gec.csv\")\n",
    "else:\n",
    "    print(\"No 2014 M2 files found! Check dataset extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617845c7",
   "metadata": {},
   "source": [
    "## 3. Step 2: Model Training (AraT5)\n",
    "Optimized for Kaggle Kernels (Local Output, No Drive Mounting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec23ee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T21:56:11.429119Z",
     "iopub.status.busy": "2026-01-12T21:56:11.428860Z",
     "iopub.status.idle": "2026-01-13T01:37:30.546914Z",
     "shell.execute_reply": "2026-01-13T01:37:30.546059Z",
     "shell.execute_reply.started": "2026-01-12T21:56:11.429100Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 21:56:32.248092: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768254992.655765      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768254992.779524      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints will be saved locally to: ./arat5-gec-checkpoints-kaggle\n",
      "Downloading starting checkpoint folder from Drive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder 1wXFxC0uN4lkbYGVCDNftaSGkeOE7sSCb checkpoint-7000\n",
      "Processing file 12XRrAGmXGOBUjxP-j9kRuWThymg3wfpP config.json\n",
      "Processing file 1_6mHmbf2iDso3n-x_4fKPfJhX6eeHt5c generation_config.json\n",
      "Processing file 10sSN6ZJzzEKIobZlaFzra-0CvlOduBXh model.safetensors\n",
      "Processing file 1Ei-O-G1t7zcon-eV8HJRWUvpule0YRnN special_tokens_map.json\n",
      "Processing file 11HbVmq-6tT2NYjDI15kTdf9FeWKwPFlL spiece.model\n",
      "Processing file 18DU2qJ8AUvBf_gVPdmaC7u4m7z5KfWlh tokenizer_config.json\n",
      "Processing file 1fZ3egkBxc8hvs0bPrbtf769Tu7EnWeCx tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=12XRrAGmXGOBUjxP-j9kRuWThymg3wfpP\n",
      "To: /kaggle/working/downloaded_starting_checkpoint/checkpoint-7000/config.json\n",
      "100%|██████████| 781/781 [00:00<00:00, 2.58MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1_6mHmbf2iDso3n-x_4fKPfJhX6eeHt5c\n",
      "To: /kaggle/working/downloaded_starting_checkpoint/checkpoint-7000/generation_config.json\n",
      "100%|██████████| 122/122 [00:00<00:00, 473kB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=10sSN6ZJzzEKIobZlaFzra-0CvlOduBXh\n",
      "From (redirected): https://drive.google.com/uc?id=10sSN6ZJzzEKIobZlaFzra-0CvlOduBXh&confirm=t&uuid=af6d8353-1e20-4b00-a05c-95985bff0843\n",
      "To: /kaggle/working/downloaded_starting_checkpoint/checkpoint-7000/model.safetensors\n",
      "100%|██████████| 1.47G/1.47G [00:11<00:00, 132MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Ei-O-G1t7zcon-eV8HJRWUvpule0YRnN\n",
      "To: /kaggle/working/downloaded_starting_checkpoint/checkpoint-7000/special_tokens_map.json\n",
      "100%|██████████| 2.54k/2.54k [00:00<00:00, 10.1MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=11HbVmq-6tT2NYjDI15kTdf9FeWKwPFlL\n",
      "To: /kaggle/working/downloaded_starting_checkpoint/checkpoint-7000/spiece.model\n",
      "100%|██████████| 2.35M/2.35M [00:00<00:00, 15.7MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=18DU2qJ8AUvBf_gVPdmaC7u4m7z5KfWlh\n",
      "To: /kaggle/working/downloaded_starting_checkpoint/checkpoint-7000/tokenizer_config.json\n",
      "100%|██████████| 21.1k/21.1k [00:00<00:00, 31.4MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1fZ3egkBxc8hvs0bPrbtf769Tu7EnWeCx\n",
      "To: /kaggle/working/downloaded_starting_checkpoint/checkpoint-7000/tokenizer.json\n",
      "100%|██████████| 15.3M/15.3M [00:00<00:00, 108MB/s] \n",
      "Download completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found requested checkpoint at: downloaded_starting_checkpoint/checkpoint-7000\n",
      "No 'trainer_state.json' found. Will LOAD WEIGHTS from this checkpoint and start fresh training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748e3cba49cd4e72b69f9e3b3070c46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from: downloaded_starting_checkpoint/checkpoint-7000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a4ce8e33294e46ab7b26daea7bc034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18385 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d96f4bc1c7470e977398e5ed74a19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training... (Resume: None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6900' max='6900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6900/6900 3:39:47, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.446500</td>\n",
       "      <td>0.316154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.414000</td>\n",
       "      <td>0.316782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.389200</td>\n",
       "      <td>0.311445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.368300</td>\n",
       "      <td>0.309877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.359400</td>\n",
       "      <td>0.310953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.307810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to ./arat5-gec-checkpoints-kaggle/arat5-gec-finetuned\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import gdown\n",
    "\n",
    "# Disable WandB explicitly\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "def run_training_step():\n",
    "    # --- Output Path (Kaggle Working Directory) ---\n",
    "    output_dir = \"./arat5-gec-checkpoints-kaggle\"\n",
    "    print(f\"Checkpoints will be saved locally to: {output_dir}\")\n",
    "\n",
    "    if not os.path.exists('qalb_full_gec.csv'):\n",
    "        print(\"Training data 'qalb_full_gec.csv' not found. Please run Step 1 Parser first.\")\n",
    "        return\n",
    "\n",
    "    # --- 1. Checkpoint Download & Setup ---\n",
    "    # NOTE: On Kaggle, you might want to skip downloading a previous checkpoint and start fresh\n",
    "    # unless you have uploaded it as a Kaggle Dataset.\n",
    "    # Below retrieves the base Arat5 model or your checkpoint logic if needed.\n",
    "    \n",
    "    start_checkpoint_url = \"https://drive.google.com/drive/folders/1Mf8XO-LgdFKgud0OoCFU1j9o9x0uB93N?usp=sharing\"\n",
    "    start_checkpoint_name = \"checkpoint-7000\"\n",
    "    download_dir = \"downloaded_starting_checkpoint\" \n",
    "    target_start_path = os.path.join(download_dir, start_checkpoint_name)\n",
    "    \n",
    "    # Download if needed\n",
    "    if not os.path.exists(target_start_path):\n",
    "        print(f\"Downloading starting checkpoint folder from Drive...\")\n",
    "        try:\n",
    "             gdown.download_folder(start_checkpoint_url, output=download_dir, quiet=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to download checkpoint: {e}\")\n",
    "\n",
    "    # --- 2. Determine Model Source ---\n",
    "    # Default base model\n",
    "    model_name = \"UBC-NLP/AraT5v2-base-1024\"\n",
    "    resume_path = None\n",
    "    \n",
    "    if os.path.exists(target_start_path):\n",
    "        print(f\"Found requested checkpoint at: {target_start_path}\")\n",
    "        # Check if it has trainer state (for resuming) or just weights (for initializing)\n",
    "        if os.path.exists(os.path.join(target_start_path, \"trainer_state.json\")):\n",
    "            print(\"Detected 'trainer_state.json'. Will RESUME training state from this checkpoint.\")\n",
    "            resume_path = target_start_path\n",
    "        else:\n",
    "            print(\"No 'trainer_state.json' found. Will LOAD WEIGHTS from this checkpoint and start fresh training.\")\n",
    "            model_name = target_start_path\n",
    "\n",
    "    # --- Data Loading ---\n",
    "    dataset = load_dataset('csv', data_files='qalb_full_gec.csv')\n",
    "    dataset = dataset['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "    # --- Model Init ---\n",
    "    print(f\"Initializing model from: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    # --- Preprocessing ---\n",
    "    prefix = \"gec_arabic: \"\n",
    "    max_input_length = 256\n",
    "    max_target_length = 256\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        inputs = [prefix + (ex if ex else \"\") for ex in examples[\"incorrect\"]]\n",
    "        targets = [(ex if ex else \"\") for ex in examples[\"correct\"]]\n",
    "\n",
    "        model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    # --- Training Config ---\n",
    "    batch_size = 2 # T4 can handle this with accumulation\n",
    "\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 1000,\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 1000,\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=4,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        num_train_epochs=6,\n",
    "        predict_with_generate=False, # DISABLED generation during eval to save time\n",
    "        fp16=True, # Enable mixed precision for T4/P100\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "    # --- Resume Logic ---\n",
    "    if resume_path is None:\n",
    "        if os.path.exists(output_dir):\n",
    "            last_checkpoint = get_last_checkpoint(output_dir)\n",
    "            if last_checkpoint:\n",
    "                print(f\"Found newer progress in output directory. Resuming from: {last_checkpoint}\")\n",
    "                resume_path = last_checkpoint\n",
    "\n",
    "    print(f\"Starting training... (Resume: {resume_path})\")\n",
    "    trainer.train(resume_from_checkpoint=resume_path)\n",
    "\n",
    "    # Save Final\n",
    "    final_path = os.path.join(output_dir, \"arat5-gec-finetuned\")\n",
    "    model.save_pretrained(final_path)\n",
    "    tokenizer.save_pretrained(final_path)\n",
    "    print(f\"Best model saved to {final_path}\")\n",
    "\n",
    "# Run\n",
    "run_training_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c25e90",
   "metadata": {},
   "source": [
    "## 4. Inference & Export\n",
    "Test the model and package it for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4933d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T01:39:00.255833Z",
     "iopub.status.busy": "2026-01-13T01:39:00.255562Z",
     "iopub.status.idle": "2026-01-13T01:40:15.159826Z",
     "shell.execute_reply": "2026-01-13T01:40:15.158986Z",
     "shell.execute_reply.started": "2026-01-13T01:39:00.255816Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping model for download...\n",
      "✅ Created arat5_gec_model_output.zip in /kaggle/working/\n",
      "You can download this file from the 'Output' tab on the right sidebar.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def package_for_kaggle_output():\n",
    "    source_dir = \"./arat5-gec-checkpoints-kaggle/arat5-gec-finetuned\"\n",
    "    output_filename = \"arat5_gec_model_output\"\n",
    "    \n",
    "    if os.path.exists(source_dir):\n",
    "        print(\"Zipping model for download...\")\n",
    "        shutil.make_archive(output_filename, 'zip', source_dir)\n",
    "        print(f\"✅ Created {output_filename}.zip in /kaggle/working/\")\n",
    "        print(\"You can download this file from the 'Output' tab on the right sidebar.\")\n",
    "    else:\n",
    "        print(\"No fine-tuned model found to zip.\")\n",
    "\n",
    "package_for_kaggle_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf2ca161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T02:22:23.936491Z",
     "iopub.status.busy": "2026-01-13T02:22:23.936182Z",
     "iopub.status.idle": "2026-01-13T02:22:25.507207Z",
     "shell.execute_reply": "2026-01-13T02:22:25.506372Z",
     "shell.execute_reply.started": "2026-01-13T02:22:23.936452Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: سئلت رئيسا الوزراء عن شؤن الموظفين واجابو بان المسؤليه تقع علي عاتق الجميع فاستعدو لبدء العمل\n",
      "Loading model from: ./arat5-gec-checkpoints-kaggle/arat5-gec-finetuned on cuda\n",
      "Corrected: سئلت رئيسا الوزراء عن شؤون الموظفين وأجابوا واجيبو بأن المسؤولية تقع على عاتق الجميع فاستعدوا لبدء العمل .\n"
     ]
    }
   ],
   "source": [
    "def run_inference(input_sentence):\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "    import torch\n",
    "    import os\n",
    "    \n",
    "    # Path where we saved the model in the previous step\n",
    "    model_path = \"./arat5-gec-checkpoints-kaggle/arat5-gec-finetuned\"\n",
    "    \n",
    "    # Fallback if training wasn't run, check for downloaded checkpoint-3000\n",
    "    if not os.path.exists(model_path):\n",
    "         # Try finding uploaded or downloaded checkpoint\n",
    "         start_ckpt = \"downloaded_starting_checkpoint/checkpoint-3000\" # Example\n",
    "         if os.path.exists(start_ckpt) and (\"model.safetensors\" in os.listdir(start_ckpt) or \"pytorch_model.bin\" in os.listdir(start_ckpt)):\n",
    "             model_path = start_ckpt\n",
    "         else:\n",
    "             print(\"Fine-tuned model not found. Using Base Model for demo.\")\n",
    "             model_path = \"UBC-NLP/AraT5v2-base-1024\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Loading model from: {model_path} on {device}\")\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    except Exception as e:\n",
    "        return f\"Error loading model: {e}\"\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Preprocessing\n",
    "    prefix = \"gec_arabic: \"\n",
    "    text = prefix + input_sentence\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, truncation=True).to(device)\n",
    "\n",
    "    # Generation\n",
    "    # Switch to Greedy Search (num_beams=1)\n",
    "    # Why? Beam search often gets stuck in loops for under-trained models because the loop has high probability.\n",
    "    # Greedy search forces the model to move forward one best-step at a time.\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=256,\n",
    "        num_beams=1, # Greedy decoding\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.2, # Increased to 2.5 (Strong penalty for stuttering)\n",
    "        no_repeat_ngram_size=3 # Mild penalty\n",
    "    )\n",
    "\n",
    "    corrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return corrected_sentence\n",
    "\n",
    "# Test\n",
    "test_sentence = \"سئلت رئيسا الوزراء عن شؤن الموظفين واجابو بان المسؤليه تقع علي عاتق الجميع فاستعدو لبدء العمل\"\n",
    "print(f\"Original: {test_sentence}\")\n",
    "print(f\"Corrected: {run_inference(test_sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4155570-df37-4650-a1f9-dc899c263171",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T02:24:32.414270Z",
     "iopub.status.busy": "2026-01-13T02:24:32.413693Z",
     "iopub.status.idle": "2026-01-13T02:24:35.420826Z",
     "shell.execute_reply": "2026-01-13T02:24:35.420191Z",
     "shell.execute_reply.started": "2026-01-13T02:24:32.414246Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:      سئلت رئيسا الوزراء عن شؤن الموظفين واجابو بان المسؤليه تقع علي عاتق الجميع فاسبعدو لبدء العم\n",
      "Loading model from: ./arat5-gec-checkpoints-kaggle/arat5-gec-finetuned on cuda\n",
      "Corrected: سئلت رئيسا الوزراء عن شؤون الموظفين وأجابوا واجيبو بأن المسؤولية تقع على عاتق الجميع فاسبعدوا لبدء العمل .\n",
      "Loading Tashkeel model: Abdou/arabic-tashkeel-flan-t5-small...\n",
      "Tashkeel:  سُئِلَتْ رَئِيسًا الْوُزَرَاءُ عَنْ شُؤُونِ الْمُوَظَّفِينَ وَأَجَابُوا وَاجِيبُو بِأَنَّ الْمَسْؤُولِيَّةَ تَقَعُ عَلَى عَاتِقِ الْجَمِيعِ فَاسْبِعُوا لِبَدْءِ الْعَمَلِ .\n"
     ]
    }
   ],
   "source": [
    "def add_tashkeel(text):\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "    import torch\n",
    "\n",
    "    # UPDATED: Using a valid public model from Hugging Face\n",
    "    # 'Abdou/arabic-tashkeel-flan-t5-small' (75MB) - highly efficient\n",
    "    tashkeel_model_name = \"Abdou/arabic-tashkeel-flan-t5-small\"\n",
    "    \n",
    "    print(f\"Loading Tashkeel model: {tashkeel_model_name}...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tashkeel_model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(tashkeel_model_name)\n",
    "    except Exception as e:\n",
    "        return f\"Error loading tashkeel model: {e}\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Prepare input (Model expects raw text without prefix)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "\n",
    "    # Generate diacritized text\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=512,\n",
    "        num_beams=4, # Recommended setting for this model\n",
    "        early_stopping=True,\n",
    "        repetition_penalty=2.5, # Increased to 2.5 (Strong penalty for stuttering)\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "\n",
    "    diacritized_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return diacritized_text\n",
    "\n",
    "# --- Full Pipeline Test ---\n",
    "test_input = \" سئلت رئيسا الوزراء عن شؤن الموظفين واجابو بان المسؤليه تقع علي عاتق الجميع فاسبعدو لبدء العم\"\n",
    "print(f\"Input:     {test_input}\")\n",
    "\n",
    "# 1. GEC (Correction)\n",
    "gec_output = run_inference(test_input)\n",
    "print(f\"Corrected: {gec_output}\")\n",
    "\n",
    "# 2. Tashkeel (Diacritization)\n",
    "final_output = add_tashkeel(gec_output)\n",
    "print(f\"Tashkeel:  {final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f107b5-32e6-4869-b1b0-5aaab5d62cde",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31240,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
