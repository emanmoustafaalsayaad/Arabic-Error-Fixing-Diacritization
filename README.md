
---

## üèÜ Model Selection

### ‚úÖ Selected Model
**`glonor/byt5-arabic-diacritization`**

- **Architecture:** ByT5 (Byte-level T5, Seq2Seq)
- **Task:** Text-to-text diacritization
- **Key Advantages:**
  - Consumes raw Arabic text directly
  - Outputs fully diacritized text
  - No need for token-level or character-level decoding logic
  - Robust to spelling variation and OOV tokens
  - Very simple inference pipeline

**Decision:** ‚úî Proceed with ByT5

---

### ‚ùå Rejected Alternatives

- **AraT5 / Shakkala (BERT / RNN-based):**
  - Output class labels instead of text
  - Require complex post-processing and decoding maps

- **Fine-Tashkeel:**
  - Very large model (~3GB)
  - Heavy storage and download overhead
  - Not practical for lightweight pipeline integration

---

## üß© Component Design

**File:** `diacritization_model.py`

### 1Ô∏è‚É£ Model Loading
- Use:
  - `AutoTokenizer`
  - `AutoModelForSeq2SeqLM`

### 2Ô∏è‚É£ Inference Procedure
- **Input:** Corrected Arabic sentence (no diacritics)
- **Processing:**  
  `model.generate(input_ids)`
- **Output:**  
  Decoded fully diacritized sentence

### 3Ô∏è‚É£ No Training Required
- This component runs in **inference-only mode**
- No fine-tuning or additional data needed

---

## üîó Integration with the Full Pipeline

The complete processing flow is:

